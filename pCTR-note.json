{"paragraphs":[{"text":"%pyspark\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions\nfrom __future__ import division","user":"anonymous","dateUpdated":"2017-04-23T15:43:21-0400","config":{"lineNumbers":false,"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":464,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1492892844428_-2054507981","id":"20170421-215501_851260905","dateCreated":"2017-04-22T16:27:24-0400","dateStarted":"2017-04-23T15:43:21-0400","dateFinished":"2017-04-23T15:43:58-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8710"},{"text":"%pyspark\nimport os.path\ndir_name = \"/Users/kylin1989/Downloads/data\"\n","user":"anonymous","dateUpdated":"2017-04-23T22:22:44-0400","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1492917148555_830710873","id":"20170422-231228_1157319022","dateCreated":"2017-04-22T23:12:28-0400","dateStarted":"2017-04-23T22:22:44-0400","dateFinished":"2017-04-23T22:22:44-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8711"},{"text":"%pyspark\ndescriptionid_tokensid = sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter=\"\\t\").load(os.path.join(dir_name, \"descriptionid_tokensid.txt\"))\ndescriptionid_tokensid.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:50:40-0400","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492916966724_-1680822677","id":"20170422-230926_1549676367","dateCreated":"2017-04-22T23:09:26-0400","dateStarted":"2017-04-23T15:46:10-0400","dateFinished":"2017-04-23T15:46:10-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8712","title":"Load descriptionid_tokensid.txt"},{"text":"%pyspark\npurchasedkeywordid_tokensid = sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter=\"\\t\").load(os.path.join(dir_name, \"purchasedkeywordid_tokensid.txt\"))\npurchasedkeywordid_tokensid.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:50:56-0400","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492917810620_-417484601","id":"20170422-232330_2027512770","dateCreated":"2017-04-22T23:23:30-0400","dateStarted":"2017-04-23T15:46:28-0400","dateFinished":"2017-04-23T15:46:28-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8713","title":"Load purchasedkeywordid_tokensid.txt"},{"text":"%pyspark\nqueryid_tokensid = sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter=\"\\t\").load(os.path.join(dir_name, \"queryid_tokensid.txt\"))\nqueryid_tokensid.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:50:51-0400","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492892844434_-2042965514","id":"20170422-142536_891748670","dateCreated":"2017-04-22T16:27:24-0400","dateStarted":"2017-04-23T15:46:29-0400","dateFinished":"2017-04-23T15:46:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8714","title":"Load queryid_tokensid.tx"},{"text":"%pyspark\ntitleid_tokensid = sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter=\"\\t\").load(os.path.join(dir_name, \"titleid_tokensid.txt\"))\ntitleid_tokensid.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:51:08-0400","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492917677429_-311060753","id":"20170422-232117_650626617","dateCreated":"2017-04-22T23:21:17-0400","dateStarted":"2017-04-23T15:46:32-0400","dateFinished":"2017-04-23T15:46:32-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8715","title":" Load titleid_tokensid.txt"},{"text":"%pyspark\nuserid_profile = sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter=\"\\t\").load(os.path.join(dir_name, \"userid_profile.txt\"))\nuserid_profile.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:51:15-0400","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492917685047_88860275","id":"20170422-232125_1561285407","dateCreated":"2017-04-22T23:21:25-0400","dateStarted":"2017-04-23T15:46:33-0400","dateFinished":"2017-04-23T15:46:34-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8716","title":"Load userid_profile.txt"},{"text":"%pyspark\ntraining = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=\"false\", inferSchema=\"true\", delimiter=\"\\t\").load(os.path.join(dir_name,\"training.txt\"))\n\ntraining = training.withColumnRenamed('_c0', 'Click').withColumnRenamed('_c1', 'Impression').withColumnRenamed('_c2', 'DisplayURL').withColumnRenamed('_c3', 'AdID').withColumnRenamed('_c4', 'AdvertiserID').withColumnRenamed('_c5', 'Depth').withColumnRenamed('_c6', 'Position').withColumnRenamed('_c7', 'QueryID').withColumnRenamed('_c8', 'KeywordID').withColumnRenamed('_c9', 'TitleID').withColumnRenamed('_c10', 'DescriptionID').withColumnRenamed('_c11', 'UserID')\ntraining.printSchema(), training.count()","user":"anonymous","dateUpdated":"2017-04-23T21:51:20-0400","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: decimal(20,0) (nullable = true)\n |-- _c3: integer (nullable = true)\n |-- _c4: integer (nullable = true)\n |-- _c5: integer (nullable = true)\n |-- _c6: integer (nullable = true)\n |-- _c7: integer (nullable = true)\n |-- _c8: integer (nullable = true)\n |-- _c9: integer (nullable = true)\n |-- _c10: integer (nullable = true)\n |-- _c11: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1492892844431_-2054123232","id":"20170421-221742_2093965047","dateCreated":"2017-04-22T16:27:24-0400","dateStarted":"2017-04-23T15:46:35-0400","dateFinished":"2017-04-23T15:52:50-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8717","title":"Load training.txt"},{"text":"%pyspark\ntest = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=\"false\", inferSchema=\"true\", delimiter=\"\\t\").load(os.path.join(dir_name,\"test.txt\"))\ntest = training.withColumnRenamed('_c0', 'DisplayURL').withColumnRenamed('_c1', 'AdID').withColumnRenamed('_c2', 'AdvertiserID').withColumnRenamed('_c3', 'Depth').withColumnRenamed('_c4', 'Position').withColumnRenamed('_c5', 'QueryID').withColumnRenamed('_c6', 'KeywordID').withColumnRenamed('_c7', 'TitleID').withColumnRenamed('_c8', 'DescriptionID').withColumnRenamed('_c9', 'UserID')\ntest.printSchema(), test.count()","user":"anonymous","dateUpdated":"2017-04-23T22:22:47-0400","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1493000444586_2092285630","id":"20170423-222044_532393071","dateCreated":"2017-04-23T22:20:44-0400","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10826","dateFinished":"2017-04-23T22:23:28-0400","dateStarted":"2017-04-23T22:22:47-0400","title":"Load test.txt","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"root\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdID: integer (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-7345277163742553851.py\", line 349, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-7345277163742553851.py\", line 342, in <module>\n    exec(code)\n  File \"<stdin>\", line 3, in <module>\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/dataframe.py\", line 380, in count\n    return int(self._jdf.count())\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1075.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 42.0 failed 1 times, most recent failure: Lost task 2.0 in stage 42.0 (TID 2693, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/kylin1989/Downloads/track2/training.txt does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:157)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2404)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File file:/Users/kylin1989/Downloads/track2/training.txt does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:157)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\n"}]}},{"text":"%md\n## PART 2 (non-Big-Data)\nidentify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.\n","user":"anonymous","dateUpdated":"2017-04-23T15:21:09-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>PART 2 (non-Big-Data)</h2>\n<p>identify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1492975190465_589883785","id":"20170423-151950_157799325","dateCreated":"2017-04-23T15:19:50-0400","dateStarted":"2017-04-23T15:21:09-0400","dateFinished":"2017-04-23T15:21:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8719"},{"text":"%pyspark\ntraining_top25k_AdID_QueryID = training.groupBy(\"AdID\", \"QueryID\").count().sort(desc(\"count\")).limit(25000)\ntraining_top25k_AdID_QueryID.printSchema()\n# print(training_top25k_AdID_QueryID.count())","user":"anonymous","dateUpdated":"2017-04-23T21:50:27-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- count: long (nullable = false)\n\n(None, 25000)\n"}]},"apps":[],"jobName":"paragraph_1492918411251_-223496179","id":"20170422-233331_1614972558","dateCreated":"2017-04-22T23:33:31-0400","dateStarted":"2017-04-23T15:59:58-0400","dateFinished":"2017-04-23T16:20:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8720","title":"Identify the top25k AdID-QueryID combination by the frequence of instances in the training dataset"},{"text":"%pyspark\ntraining_top25k_AdID_QueryID.drop(\"count\")\ntraining_top25K = training.join(training_top25k_AdID_QueryID, ['AdID', 'QueryID'], 'inner')\ntraining_top25K.printSchema()\n# print(training_top25K.count())","user":"anonymous","dateUpdated":"2017-04-23T21:50:13-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n |-- count: long (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1492975731673_-1468556768","id":"20170423-152851_2110849751","dateCreated":"2017-04-23T15:28:51-0400","dateStarted":"2017-04-23T17:51:28-0400","dateFinished":"2017-04-23T17:51:28-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8721","title":"Filter the training using the identified the top25k AdID-QueryID combinations"},{"text":"%pyspark\ntraining_top25K.coalesce(1).write.format(\"com.databricks.spark.csv\").options(header=\"true\").save(os.path.join(dir_name,\"training_top25K\"))","user":"anonymous","dateUpdated":"2017-04-23T21:50:01-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492984145416_-1659179402","id":"20170423-174905_286254460","dateCreated":"2017-04-23T17:49:05-0400","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9113","dateFinished":"2017-04-23T17:56:59-0400","dateStarted":"2017-04-23T17:51:59-0400","errorMessage":"","title":"Save training_top25K"},{"text":"%md\n## PART 3 \nCompute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.\n","user":"anonymous","dateUpdated":"2017-04-23T21:05:45-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>PART 3</h2>\n<p>Compute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1492975732832_-646348368","id":"20170423-152852_1407717236","dateCreated":"2017-04-23T15:28:52-0400","dateStarted":"2017-04-23T21:05:45-0400","dateFinished":"2017-04-23T21:05:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8723"},{"text":"%pyspark\ntraining_top25K_CTR = training_top25K.withColumn('CTR', training_top25K.Click/training_top25K.Impression)\ntraining_top25K_CTR.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:49:51-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492996033107_-2032875988","id":"20170423-210713_754289250","dateCreated":"2017-04-23T21:07:13-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9556","dateFinished":"2017-04-23T21:44:36-0400","dateStarted":"2017-04-23T21:44:35-0400","title":"Add CTR column","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n |-- count: long (nullable = false)\n |-- CTR: double (nullable = true)\n\n"}]}},{"text":"%pyspark\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(\n    inputCols=[\"CTR\", \"Position\", \"Depth\"],\n    outputCol=\"features\")\ntraining_top25K_assembled = assembler.transform(training_top25K_CTR)\ntraining_top25K_assembled.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:49:45-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492996980354_326707986","id":"20170423-212300_811037492","dateCreated":"2017-04-23T21:23:00-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9830","dateFinished":"2017-04-23T21:44:53-0400","dateStarted":"2017-04-23T21:44:52-0400","title":"Assemble CTR, Position and Depth for normalization","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n |-- count: long (nullable = false)\n |-- CTR: double (nullable = true)\n |-- features: vector (nullable = true)\n\n"}]}},{"text":"%pyspark\nfrom pyspark.ml.feature import Normalizer\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\ntraining_top25K_normalized = normalizer.transform(training_top25K_assembled)\ntraining_top25K_normalized.printSchema()","user":"anonymous","dateUpdated":"2017-04-23T21:49:31-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492997326177_1578246373","id":"20170423-212846_196516663","dateCreated":"2017-04-23T21:28:46-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10027","dateFinished":"2017-04-23T21:45:51-0400","dateStarted":"2017-04-23T21:45:51-0400","title":"Normalize features (CTR, Position and Depth)","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n |-- count: long (nullable = false)\n |-- CTR: double (nullable = true)\n |-- features: vector (nullable = true)\n |-- normFeatures: vector (nullable = true)\n\n"}]}},{"text":"%md\n## PART 4\nAnnotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.","user":"anonymous","dateUpdated":"2017-04-23T21:05:53-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492995915775_790241231","id":"20170423-210515_1452637488","dateCreated":"2017-04-23T21:05:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9312","dateFinished":"2017-04-23T21:05:53-0400","dateStarted":"2017-04-23T21:05:53-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>PART 4</h2>\n<p>Annotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.</p>\n</div>"}]}},{"text":"%pyspark\n# from pyspark.ml.feature import VectorSlicer\ndef extract_normCTR(normFeature):\n    return normFeature.values[0].item() \nextract_normCTR_udf = functions.udf(extract_normCTR, FloatType())\n\ntraining_top25K_normalized_udf = training_top25K_normalized.select('*', extract_normCTR_udf(training_top25K_normalized.normFeatures).alias('extractNormCTRudf'))\ntraining_top25K_normCTR = training_top25K_normalized_udf.withColumn('normCTR', training_top25K_normalized_udf.extractNormCTRudf).drop('extractNormCTRudf')\ntraining_top25K_normCTR.printSchema()\ntraining_top25K_normCTR.show(1)","user":"anonymous","dateUpdated":"2017-04-23T22:07:45-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492997309913_-643324425","id":"20170423-212829_396942010","dateCreated":"2017-04-23T21:28:29-0400","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9971","dateFinished":"2017-04-23T22:12:46-0400","dateStarted":"2017-04-23T22:07:45-0400","title":"Extract the normalized CTR","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"root\n |-- AdID: integer (nullable = true)\n |-- QueryID: integer (nullable = true)\n |-- Click: integer (nullable = true)\n |-- Impression: integer (nullable = true)\n |-- DisplayURL: decimal(20,0) (nullable = true)\n |-- AdvertiserID: integer (nullable = true)\n |-- Depth: integer (nullable = true)\n |-- Position: integer (nullable = true)\n |-- KeywordID: integer (nullable = true)\n |-- TitleID: integer (nullable = true)\n |-- DescriptionID: integer (nullable = true)\n |-- UserID: integer (nullable = true)\n |-- count: long (nullable = false)\n |-- CTR: double (nullable = true)\n |-- features: vector (nullable = true)\n |-- normFeatures: vector (nullable = true)\n |-- normCTR: float (nullable = true)\n\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-7345277163742553851.py\", line 349, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-7345277163742553851.py\", line 342, in <module>\n    exec(code)\n  File \"<stdin>\", line 7, in <module>\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/dataframe.py\", line 318, in show\n    print(self._jdf.showString(n, 20))\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o924.showString.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResultInForkJoinSafely(ThreadUtils.scala:215)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:131)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:209)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:141)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:333)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:141)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:128)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:313)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:354)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:235)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:368)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:308)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResultInForkJoinSafely(ThreadUtils.scala:212)\n\t... 100 more\n\n\n"}]}},{"text":"%md\n## PART 5\nShape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.","user":"anonymous","dateUpdated":"2017-04-23T21:05:34-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492977005280_504455850","id":"20170423-155005_46408624","dateCreated":"2017-04-23T15:50:05-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8724","dateFinished":"2017-04-23T21:05:35-0400","dateStarted":"2017-04-23T21:05:34-0400","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>PART 5</h2>\n<p>Shape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.</p>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2017-04-23T21:05:34-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492995934848_-1688095001","id":"20170423-210534_372251776","dateCreated":"2017-04-23T21:05:34-0400","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9408"}],"name":"pCTR/note","id":"2CGDKZJCG","angularObjects":{"2CF6E122K:shared_process":[],"2CH1G59ET:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{},"checkpoint":{"message":""}}