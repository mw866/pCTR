{
  "paragraphs": [
    {
      "text": "%md\n# CS 5304 Assignments #3\nBy Chris M Wang (mw866@cornell.edu)",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 1:55:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eCS 5304 Assignments #3\u003c/h1\u003e\n\u003cp\u003eBy Chris M Wang (\u003ca href\u003d\"mailto:\u0026#109;w\u0026#56;\u0026#x36;\u0026#x36;\u0026#x40;\u0026#x63;\u0026#111;\u0026#x72;\u0026#x6e;e\u0026#108;\u0026#x6c;\u0026#46;e\u0026#100;\u0026#117;\"\u003e\u0026#109;w\u0026#56;\u0026#x36;\u0026#x36;\u0026#x40;\u0026#x63;\u0026#111;\u0026#x72;\u0026#x6e;e\u0026#108;\u0026#x6c;\u0026#46;e\u0026#100;\u0026#117;\u003c/a\u003e)\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493315700403_-63469888",
      "id": "20170427-135500_1483027983",
      "dateCreated": "Apr 27, 2017 1:55:00 PM",
      "dateStarted": "Apr 27, 2017 1:55:35 PM",
      "dateFinished": "Apr 27, 2017 1:55:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 1: Use machine learning to predict the click-through-rate. \nPerformance for this task should be measured using the Area Under Curve (AUC) metric. In short, the AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The AUC results should be computed for the training set and for test set that has about 2 million examples. Describe the experimental setup and discuss bias- variance trade-offs in your experimental design. Submit the code and the predictions for the training set and the test set.",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:06:07 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 1: Use machine learning to predict the click-through-rate.\u003c/h2\u003e\n\u003cp\u003ePerformance for this task should be measured using the Area Under Curve (AUC) metric. In short, the AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The AUC results should be computed for the training set and for test set that has about 2 million examples. Describe the experimental setup and discuss bias- variance trade-offs in your experimental design. Submit the code and the predictions for the training set and the test set.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493496352689_177513462",
      "id": "20170429-160552_1472663596",
      "dateCreated": "Apr 29, 2017 4:05:52 PM",
      "dateStarted": "Apr 29, 2017 4:06:07 PM",
      "dateFinished": "Apr 29, 2017 4:06:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions\nfrom __future__ import division\n\nimport os.path\ndir_name \u003d \"/Users/kylin1989/Downloads/data\"",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 3:51:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493315735204_993866062",
      "id": "20170427-135535_324419007",
      "dateCreated": "Apr 27, 2017 1:55:35 PM",
      "dateStarted": "Apr 29, 2017 3:51:28 PM",
      "dateFinished": "Apr 29, 2017 3:52:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize spark session",
      "text": "%pyspark\nspark \u003d SparkSession.builder.appName(\"pCTR\").config(\"spark.sql.broadcastTimeout\", \"6000\").getOrCreate()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:00:25 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493315771833_2037167162",
      "id": "20170427-135611_692908824",
      "dateCreated": "Apr 27, 2017 1:56:11 PM",
      "dateStarted": "Apr 29, 2017 4:00:25 PM",
      "dateFinished": "Apr 29, 2017 4:00:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load training parquet",
      "text": "%pyspark\ntrain \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"training_top25K_normCTR_assembled\"))\ntrain",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:00:43 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493315848653_-1728446944",
      "id": "20170427-135728_48369402",
      "dateCreated": "Apr 27, 2017 1:57:28 PM",
      "dateStarted": "Apr 29, 2017 4:00:43 PM",
      "dateFinished": "Apr 29, 2017 4:00:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Split Train and Test in training dataset",
      "text": "%pyspark\ntrain_train, train_test \u003d train.randomSplit([0.9, 0.1], seed\u003d10011)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:01:04 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493319034436_-1931292290",
      "id": "20170427-145034_1611025369",
      "dateCreated": "Apr 27, 2017 2:50:34 PM",
      "dateStarted": "Apr 29, 2017 4:00:53 PM",
      "dateFinished": "Apr 29, 2017 4:00:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize and train the regression model in a fast-and-dirty way",
      "text": "%pyspark\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\nlinr \u003d LinearRegression(maxIter\u003d10, regParam\u003d0.3, elasticNetParam\u003d0.8, labelCol\u003d\"normCTR\", featuresCol\u003d\"features\", predictionCol\u003d\"prediction\",)\nlinrModel \u003d linr.fit(train_train)\nlinrModel.summary",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:51:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 337, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 5, in \u003cmodule\u003e\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/base.py\", line 64, in fit\n    return self._fit(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 236, in _fit\n    java_model \u003d self._fit_java(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 233, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o56.fit.\n: org.apache.spark.SparkException: Job 4 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1088)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1082)\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply$mcD$sp(DoubleRDDFunctions.scala:35)\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)\n\tat org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.sum(DoubleRDDFunctions.scala:34)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSreg$lzycompute(RegressionMetrics.scala:71)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSreg(RegressionMetrics.scala:67)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.explainedVariance(RegressionMetrics.scala:82)\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.\u003cinit\u003e(LinearRegression.scala:632)\n\tat org.apache.spark.ml.regression.LinearRegressionTrainingSummary.\u003cinit\u003e(LinearRegression.scala:575)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:220)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:76)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493319524798_-344065159",
      "id": "20170427-145844_497523205",
      "dateCreated": "Apr 27, 2017 2:58:44 PM",
      "dateStarted": "Apr 29, 2017 4:01:32 PM",
      "dateFinished": "Apr 29, 2017 4:14:28 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save regression model",
      "text": "%pyspark\nlinrModel.save(os.path.join(dir_name, \u0027linrModel\u0027))",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:51:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493495413444_2073242980",
      "id": "20170429-155013_1734071651",
      "dateCreated": "Apr 29, 2017 3:50:13 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load regression model",
      "text": "%pyspark\nlinrModel \u003d LinearRegression.load(\"linrModel\")",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:51:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493495433425_-163656477",
      "id": "20170429-155033_313778734",
      "dateCreated": "Apr 29, 2017 3:50:33 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Predict the train - test data",
      "text": "%pyspark\ntrain_test_predictions \u003d linrModel.transform(train_test)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:51:25 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493319754718_1388396197",
      "id": "20170427-150234_751753614",
      "dateCreated": "Apr 27, 2017 3:02:34 PM",
      "dateStarted": "Apr 27, 2017 3:35:23 PM",
      "dateFinished": "Apr 27, 2017 3:35:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Binarize normCTR of train",
      "text": "%pyspark\nfrom pyspark.ml.feature import Binarizer\n\nbinarizer \u003d Binarizer(threshold\u003d0.0, inputCol\u003d\"normCTR\", outputCol\u003d\"binarized_normCTR\")\ntrain_train_binarized \u003d binarizer.transform(train_train)\ntrain_train_binarized",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:36:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector, binarized_normCTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493497361258_721961039",
      "id": "20170429-162241_33935927",
      "dateCreated": "Apr 29, 2017 4:22:41 PM",
      "dateStarted": "Apr 29, 2017 4:24:36 PM",
      "dateFinished": "Apr 29, 2017 4:24:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Binarize normCTR of test",
      "text": "%pyspark\ntrain_test_binarized \u003d binarizer.transform(train_test)\ntrain_test_binarized",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 6:37:02 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector, binarized_normCTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493501214372_-1315939514",
      "id": "20170429-172654_319671991",
      "dateCreated": "Apr 29, 2017 5:26:54 PM",
      "dateStarted": "Apr 29, 2017 5:27:30 PM",
      "dateFinished": "Apr 29, 2017 5:27:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize and train the classification model for AUC",
      "text": "%pyspark\nfrom pyspark.ml.classification import LogisticRegression\n\n# logr \u003d LogisticRegression(maxIter\u003d10, regParam\u003d0.3, elasticNetParam\u003d0.8, featuresCol\u003d\"features\", labelCol\u003d\"binarized_normCTR\" )\nlogr \u003d LogisticRegression(featuresCol\u003d\"features\", labelCol\u003d\"binarized_normCTR\" )\nlogrModel \u003d logr.fit(train_train_binarized)\nlogrModel",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:17:36 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 93.0,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "LogisticRegression_4293945c258c3339a78b\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493496525054_775057508",
      "id": "20170429-160845_1637890393",
      "dateCreated": "Apr 29, 2017 4:08:45 PM",
      "dateStarted": "Apr 29, 2017 6:46:22 PM",
      "dateFinished": "Apr 29, 2017 6:55:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Use trained classification model to predict test data",
      "text": "%pyspark\ntrain_test_predictions \u003d logrModel.transform(train_test_binarized)\ntrain_test_predictions",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:18:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector, binarized_normCTR: double, rawPrediction: vector, probability: vector, prediction: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493499876760_1437996234",
      "id": "20170429-170436_2090008023",
      "dateCreated": "Apr 29, 2017 5:04:36 PM",
      "dateStarted": "Apr 29, 2017 5:28:18 PM",
      "dateFinished": "Apr 29, 2017 5:28:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ntrain_test_predictions.show(5)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:24:14 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+--------------------+-----------------+--------------------+--------------------+----------+\n|normCTR|            features|binarized_normCTR|       rawPrediction|         probability|prediction|\n+-------+--------------------+-----------------+--------------------+--------------------+----------+\n|    0.0|[1000467.0,9458.0...|              0.0|[2.73647265594046...|[0.93914481450741...|       0.0|\n|    0.0|[1000515.0,1805.0...|              0.0|[2.73647265594046...|[0.93914481450741...|       0.0|\n|    0.0|[1000515.0,1805.0...|              0.0|[2.73647265594046...|[0.93914481450741...|       0.0|\n|    0.0|[1000515.0,1805.0...|              0.0|[2.73647265594046...|[0.93914481450741...|       0.0|\n|    0.0|[1000515.0,1805.0...|              0.0|[2.73647265594046...|[0.93914481450741...|       0.0|\n+-------+--------------------+-----------------+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493500541041_-2112167234",
      "id": "20170429-171541_1197023242",
      "dateCreated": "Apr 29, 2017 5:15:41 PM",
      "dateStarted": "Apr 29, 2017 7:24:14 PM",
      "dateFinished": "Apr 29, 2017 7:24:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Evaluate AUC",
      "text": "%pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator \u003d BinaryClassificationEvaluator(labelCol\u003d\"binarized_normCTR\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\nevaluator.evaluate(train_test_predictions)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:31:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "0.5\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493321571846_-729688658",
      "id": "20170427-153251_234185670",
      "dateCreated": "Apr 27, 2017 3:32:51 PM",
      "dateStarted": "Apr 29, 2017 7:31:31 PM",
      "dateFinished": "Apr 29, 2017 7:37:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize and train the model with tuning and cross-validation",
      "text": "%pyspark\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nlr \u003d LinearRegression(maxIter\u003d10, regParam\u003d0.3, elasticNetParam\u003d0.8, labelCol\u003d\"normCTR\", featuresCol\u003d\"features\", predictionCol\u003d\"prediction\",)\n\nparamGrid \u003d ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.fitIntercept, [False, True])\\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n    .build()\n\ntvs \u003d TrainValidationSplit(estimator\u003dlr,\n                           estimatorParamMaps\u003dparamGrid,\n                           evaluator\u003dRegressionEvaluator(),\n                           # 80% of the data will be used for training, 20% for validation.\n                           trainRatio\u003d0.8)\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel \u003d tvs.fit(train_train)\n\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel.transform(train_test)",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 3:04:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {},
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-1198231712144521154.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-1198231712144521154.py\", line 337, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 14, in \u003cmodule\u003e\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/base.py\", line 64, in fit\n    return self._fit(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/tuning.py\", line 392, in _fit\n    model \u003d est.fit(train, epm[j])\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/base.py\", line 62, in fit\n    return self.copy(params)._fit(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 236, in _fit\n    java_model \u003d self._fit_java(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 233, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o276.fit.\n: org.apache.spark.SparkException: Job 11 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:100)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:215)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:76)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493315917877_-1266307676",
      "id": "20170427-135837_684025648",
      "dateCreated": "Apr 27, 2017 1:58:37 PM",
      "dateStarted": "Apr 27, 2017 2:57:37 PM",
      "dateFinished": "Apr 27, 2017 3:00:12 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 2: Conduct an error analysis of the model’s predictions on the training and testing set. \nDescribe limitations of the model exposed by the analysis.\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:07:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 2: Conduct an error analysis of the model’s predictions on the training and testing set.\u003c/h2\u003e\n\u003cp\u003eDescribe limitations of the model exposed by the analysis.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493496406528_-2055676423",
      "id": "20170429-160646_735770069",
      "dateCreated": "Apr 29, 2017 4:06:46 PM",
      "dateStarted": "Apr 29, 2017 4:07:01 PM",
      "dateFinished": "Apr 29, 2017 4:07:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 3: Suggest a modification to the training set to improve model performance. \nJustify using the error analysis.",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:55:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 3: Suggest a modification to the training set to improve model performance.\u003c/h2\u003e\n\u003cp\u003eJustify using the error analysis.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493499295097_-1966693279",
      "id": "20170429-165455_801493034",
      "dateCreated": "Apr 29, 2017 4:54:55 PM",
      "dateStarted": "Apr 29, 2017 4:55:06 PM",
      "dateFinished": "Apr 29, 2017 4:55:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 4: Implement the suggested feature change to the original model, report results, and compare the performance to the original model. \nSubmit the code and the data sets with the new predictions for the training and test set.",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:55:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 4: Implement the suggested feature change to the original model, report results, and compare the performance to the original model.\u003c/h2\u003e\n\u003cp\u003eSubmit the code and the data sets with the new predictions for the training and test set.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493499314394_-864910347",
      "id": "20170429-165514_697805990",
      "dateCreated": "Apr 29, 2017 4:55:14 PM",
      "dateStarted": "Apr 29, 2017 4:55:22 PM",
      "dateFinished": "Apr 29, 2017 4:55:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Reference\n* A Two-Stage Ensemble of Diverse Models for Advertisement Ranking in KDD Cup 2012",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 4:07:30 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eReference\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eA Two-Stage Ensemble of Diverse Models for Advertisement Ranking in KDD Cup 2012\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493318987364_963927336",
      "id": "20170427-144947_1274782457",
      "dateCreated": "Apr 27, 2017 2:49:47 PM",
      "dateStarted": "Apr 29, 2017 4:07:30 PM",
      "dateFinished": "Apr 29, 2017 4:07:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load test parquet",
      "text": "%pyspark\ntest \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"test_assembled\"))\ntest",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 2:19:19 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493317140571_1861129478",
      "id": "20170427-141900_485884562",
      "dateCreated": "Apr 27, 2017 2:19:00 PM",
      "dateStarted": "Apr 27, 2017 2:19:19 PM",
      "dateFinished": "Apr 27, 2017 2:19:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/pCTR/HW3",
  "id": "2CF1KCXC7",
  "angularObjects": {
    "2CGRCWS7B:shared_process": [],
    "2CEJ2P2FV:shared_process": [],
    "2CGD89D7T:shared_process": [],
    "2CDR262YF:shared_process": [],
    "2CGD88WGM:shared_process": [],
    "2CE3RQP7C:shared_process": [],
    "2CF6E122K:shared_process": [],
    "2CH1G59ET:shared_process": []
  },
  "config": {},
  "info": {}
}