{
  "paragraphs": [
    {
      "text": "%md\n# CS 5304 Assignments #2\nBy Chris M Wang (mw866@cornell.edu)",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 2:30:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eCS 5304 Assignments #2\u003c/h1\u003e\n\u003cp\u003eBy Chris M Wang (\u003ca href\u003d\"mailto:\u0026#109;w\u0026#56;6\u0026#x36;\u0026#64;\u0026#x63;\u0026#x6f;\u0026#114;\u0026#110;e\u0026#x6c;\u0026#x6c;.\u0026#101;\u0026#x64;\u0026#x75;\"\u003e\u0026#109;w\u0026#56;6\u0026#x36;\u0026#64;\u0026#x63;\u0026#x6f;\u0026#114;\u0026#110;e\u0026#x6c;\u0026#x6c;.\u0026#101;\u0026#x64;\u0026#x75;\u003c/a\u003e)\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493005023932_-293168309",
      "id": "20170423-233703_1256560744",
      "dateCreated": "Apr 23, 2017 11:37:03 PM",
      "dateStarted": "Apr 27, 2017 2:30:44 PM",
      "dateFinished": "Apr 27, 2017 2:30:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "%pyspark\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions\nfrom __future__ import division\n\nimport os.path\ndir_name \u003d \"/Users/kylin1989/Downloads/data\"",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:56:43 PM",
      "config": {
        "lineNumbers": false,
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 464.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1492892844428_-2054507981",
      "id": "20170421-215501_851260905",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 29, 2017 7:56:44 PM",
      "dateFinished": "Apr 29, 2017 7:56:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize spark session",
      "text": "%pyspark\nspark \u003d SparkSession.builder.appName(\"pCTR\").config(\"spark.sql.broadcastTimeout\", \"6000\").getOrCreate()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:56:47 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493002740890_-250572492",
      "id": "20170423-225900_1904584632",
      "dateCreated": "Apr 23, 2017 10:59:00 PM",
      "dateStarted": "Apr 29, 2017 7:56:47 PM",
      "dateFinished": "Apr 29, 2017 7:56:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load descriptionid_tokensid.txt",
      "text": "%pyspark\ndescriptionid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"descriptionid_tokensid.txt\"))\ndescriptionid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 2:31:34 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492916966724_-1680822677",
      "id": "20170422-230926_1549676367",
      "dateCreated": "Apr 22, 2017 11:09:26 PM",
      "dateStarted": "Apr 27, 2017 2:31:34 PM",
      "dateFinished": "Apr 27, 2017 2:31:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load purchasedkeywordid_tokensid.txt",
      "text": "%pyspark\npurchasedkeywordid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"purchasedkeywordid_tokensid.txt\"))\npurchasedkeywordid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 3:23:42 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917810620_-417484601",
      "id": "20170422-232330_2027512770",
      "dateCreated": "Apr 22, 2017 11:23:30 PM",
      "dateStarted": "Apr 23, 2017 10:37:28 PM",
      "dateFinished": "Apr 23, 2017 10:37:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load queryid_tokensid.tx",
      "text": "%pyspark\nqueryid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"queryid_tokensid.txt\"))\nqueryid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 3:23:43 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python"
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492892844434_-2042965514",
      "id": "20170422-142536_891748670",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 23, 2017 10:37:27 PM",
      "dateFinished": "Apr 23, 2017 10:37:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": " Load titleid_tokensid.txt",
      "text": "%pyspark\ntitleid_tokensid \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"titleid_tokensid.txt\"))\ntitleid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 10:37:32 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917677429_-311060753",
      "id": "20170422-232117_650626617",
      "dateCreated": "Apr 22, 2017 11:21:17 PM",
      "dateStarted": "Apr 23, 2017 10:37:32 PM",
      "dateFinished": "Apr 23, 2017 10:37:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load userid_profile.txt",
      "text": "%pyspark\nuserid_profile \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"userid_profile.txt\"))\nuserid_profile",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 10:37:26 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string, _c2: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917685047_88860275",
      "id": "20170422-232125_1561285407",
      "dateCreated": "Apr 22, 2017 11:21:25 PM",
      "dateStarted": "Apr 23, 2017 10:37:26 PM",
      "dateFinished": "Apr 23, 2017 10:37:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load training.txt",
      "text": "%pyspark\ntraining \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(header\u003d\"false\", inferSchema\u003d\"true\", delimiter\u003d\"\\t\").load(os.path.join(dir_name,\"training.txt\"))\n\ntraining \u003d training.withColumnRenamed(\u0027_c0\u0027, \u0027Click\u0027).withColumnRenamed(\u0027_c1\u0027, \u0027Impression\u0027).withColumnRenamed(\u0027_c2\u0027, \u0027DisplayURL\u0027).withColumnRenamed(\u0027_c3\u0027, \u0027AdID\u0027).withColumnRenamed(\u0027_c4\u0027, \u0027AdvertiserID\u0027).withColumnRenamed(\u0027_c5\u0027, \u0027Depth\u0027).withColumnRenamed(\u0027_c6\u0027, \u0027Position\u0027).withColumnRenamed(\u0027_c7\u0027, \u0027QueryID\u0027).withColumnRenamed(\u0027_c8\u0027, \u0027KeywordID\u0027).withColumnRenamed(\u0027_c9\u0027, \u0027TitleID\u0027).withColumnRenamed(\u0027_c10\u0027, \u0027DescriptionID\u0027).withColumnRenamed(\u0027_c11\u0027, \u0027UserID\u0027)\ntraining",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:09:22 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python"
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[Click: int, Impression: int, DisplayURL: decimal(20,0), AdID: int, AdvertiserID: int, Depth: int, Position: int, QueryID: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492892844431_-2054123232",
      "id": "20170421-221742_2093965047",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 29, 2017 8:09:22 PM",
      "dateFinished": "Apr 29, 2017 8:16:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load test.txt",
      "text": "%pyspark\ntest \u003d spark.read.format(\"com.databricks.spark.csv\").options(header\u003d\"false\", inferSchema\u003d\"true\", delimiter\u003d\"\\t\").load(os.path.join(dir_name,\"test.txt\"))\ntest \u003d training.withColumnRenamed(\u0027_c0\u0027, \u0027DisplayURL\u0027).withColumnRenamed(\u0027_c1\u0027, \u0027AdID\u0027).withColumnRenamed(\u0027_c2\u0027, \u0027AdvertiserID\u0027).withColumnRenamed(\u0027_c3\u0027, \u0027Depth\u0027).withColumnRenamed(\u0027_c4\u0027, \u0027Position\u0027).withColumnRenamed(\u0027_c5\u0027, \u0027QueryID\u0027).withColumnRenamed(\u0027_c6\u0027, \u0027KeywordID\u0027).withColumnRenamed(\u0027_c7\u0027, \u0027TitleID\u0027).withColumnRenamed(\u0027_c8\u0027, \u0027DescriptionID\u0027).withColumnRenamed(\u0027_c9\u0027, \u0027UserID\u0027)\ntest",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 7:56:59 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python"
        },
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[Click: int, Impression: int, DisplayURL: decimal(20,0), AdID: int, AdvertiserID: int, Depth: int, Position: int, QueryID: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493000444586_2092285630",
      "id": "20170423-222044_532393071",
      "dateCreated": "Apr 23, 2017 10:20:44 PM",
      "dateStarted": "Apr 29, 2017 7:56:59 PM",
      "dateFinished": "Apr 29, 2017 8:04:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 2 (non-Big-Data)\nidentify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.\n",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 3:21:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 2 (non-Big-Data)\u003c/h2\u003e\n\u003cp\u003eidentify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975190465_589883785",
      "id": "20170423-151950_157799325",
      "dateCreated": "Apr 23, 2017 3:19:50 PM",
      "dateStarted": "Apr 23, 2017 3:21:09 PM",
      "dateFinished": "Apr 23, 2017 3:21:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Identify the top25k AdID-QueryID combination by the frequence of instances in the training dataset",
      "text": "%pyspark\ntraining_top25k_AdID_QueryID \u003d training.groupBy(\"AdID\", \"QueryID\").count().sort(desc(\"count\")).limit(25000)\ntraining_top25k_AdID_QueryID\n# print(training_top25k_AdID_QueryID.count())",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:16:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, count: bigint]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492918411251_-223496179",
      "id": "20170422-233331_1614972558",
      "dateCreated": "Apr 22, 2017 11:33:31 PM",
      "dateStarted": "Apr 29, 2017 8:16:57 PM",
      "dateFinished": "Apr 29, 2017 8:16:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Filter the training using the identified the top25k AdID-QueryID combinations",
      "text": "%pyspark\ntraining_top25K \u003d training.join(training_top25k_AdID_QueryID.drop(\"count\"), [\u0027AdID\u0027, \u0027QueryID\u0027], \u0027inner\u0027)\ntraining_top25K",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:17:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975731673_-1468556768",
      "id": "20170423-152851_2110849751",
      "dateCreated": "Apr 23, 2017 3:28:51 PM",
      "dateStarted": "Apr 29, 2017 8:17:00 PM",
      "dateFinished": "Apr 29, 2017 8:17:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 3 \nCompute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.\n",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 9:05:45 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 3\u003c/h2\u003e\n\u003cp\u003eCompute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975732832_-646348368",
      "id": "20170423-152852_1407717236",
      "dateCreated": "Apr 23, 2017 3:28:52 PM",
      "dateStarted": "Apr 23, 2017 9:05:45 PM",
      "dateFinished": "Apr 23, 2017 9:05:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Add CTR column",
      "text": "%pyspark\ntraining_top25K_CTR \u003d training_top25K.withColumn(\u0027CTR\u0027, training_top25K.Click/training_top25K.Impression)\ntraining_top25K_CTR",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:17:04 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int, CTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492996033107_-2032875988",
      "id": "20170423-210713_754289250",
      "dateCreated": "Apr 23, 2017 9:07:13 PM",
      "dateStarted": "Apr 29, 2017 8:17:04 PM",
      "dateFinished": "Apr 29, 2017 8:17:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create NormCTR column",
      "text": "%pyspark\ntraining_top25K_normCTR \u003d training_top25K_CTR.withColumn(\u0027normCTR\u0027, training_top25K_CTR.CTR * training_top25K_CTR.Position / training_top25K_CTR.Depth).drop(\"CTR\")\ntraining_top25K_normCTR",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:17:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int, normCTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493256443156_-276304665",
      "id": "20170426-212723_821672347",
      "dateCreated": "Apr 26, 2017 9:27:23 PM",
      "dateStarted": "Apr 29, 2017 8:17:06 PM",
      "dateFinished": "Apr 29, 2017 8:17:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 4\nAnnotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 9:05:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 4\u003c/h2\u003e\n\u003cp\u003eAnnotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492995915775_790241231",
      "id": "20170423-210515_1452637488",
      "dateCreated": "Apr 23, 2017 9:05:15 PM",
      "dateStarted": "Apr 23, 2017 9:05:53 PM",
      "dateFinished": "Apr 23, 2017 9:05:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save training dataset for ML",
      "text": "%pyspark\ntraining_top25K_normCTR.coalesce(1).write.format(\"com.databricks.spark.csv\").options(header\u003d\"true\").save(os.path.join(dir_name,\"training_top25K_normCTR\"))",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:17:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 342, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/readwriter.py\", line 550, in save\n    self._jwrite.save(path)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1092.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 119 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)\n\t... 30 more\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493003743413_-1144707034",
      "id": "20170423-231543_1345825894",
      "dateCreated": "Apr 23, 2017 11:15:43 PM",
      "dateStarted": "Apr 29, 2017 8:17:59 PM",
      "dateFinished": "Apr 29, 2017 8:46:46 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load training dataset if needed",
      "text": "%pyspark\ntraining_top25K_normCTR \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"training_top25K_normCTR\"))\ntraining_top25K_normCTR",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:31:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493510593933_-904385039",
      "id": "20170429-200313_553302843",
      "dateCreated": "Apr 29, 2017 8:03:13 PM",
      "dateStarted": "Apr 29, 2017 8:03:56 PM",
      "dateFinished": "Apr 29, 2017 8:04:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save test dataset for ML",
      "text": "%pyspark\ntest.coalesce(1).write.format(\"com.databricks.spark.csv\").options(header\u003d\"true\").save(os.path.join(dir_name,\"test\"))",
      "user": "anonymous",
      "dateUpdated": "Apr 26, 2017 11:07:30 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493262450120_2015223605",
      "id": "20170426-230730_1550933375",
      "dateCreated": "Apr 26, 2017 11:07:30 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load test dataset if needed",
      "text": "%pyspark\ntest \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"test\"))\ntest",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:31:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493510925277_73316111",
      "id": "20170429-200845_367382228",
      "dateCreated": "Apr 29, 2017 8:08:45 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 5\nShape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.",
      "user": "anonymous",
      "dateUpdated": "Apr 23, 2017 9:05:34 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 5\u003c/h2\u003e\n\u003cp\u003eShape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492977005280_504455850",
      "id": "20170423-155005_46408624",
      "dateCreated": "Apr 23, 2017 3:50:05 PM",
      "dateStarted": "Apr 23, 2017 9:05:34 PM",
      "dateFinished": "Apr 23, 2017 9:05:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Answer\nI select the Linear Regression model for the following reasons:\n* the pCTR is a float, this is a regression problem\n* logistic regression has a decent performance",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 2:54:58 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eAnswer\u003c/h3\u003e\n\u003cp\u003eI select the Linear Regression model for the following reasons:\u003cbr/\u003e* the pCTR is a float, this is a regression problem\u003cbr/\u003e* logistic regression has a decent performance\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492995934848_-1688095001",
      "id": "20170423-210534_372251776",
      "dateCreated": "Apr 23, 2017 9:05:34 PM",
      "dateStarted": "Apr 27, 2017 2:54:58 PM",
      "dateFinished": "Apr 27, 2017 2:54:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create string indexer and one-hot-encoder",
      "text": "%pyspark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\ndef stringindexonehotencode(df):\n    string_cols \u003d [\"DisplayURL\", \"AdID\", \"AdvertiserID\", \"QueryID\", \"KeywordID\",  \"TitleID\", \"DescriptionID\", \"UserID\"]\n    \n    for col in string_cols:\n        string_indexer \u003d StringIndexer(inputCol\u003dcol, outputCol\u003d(col + \"_index\"))\n        string_indexer.fit(df)\n    \n        df \u003d str_index.transform(df)\n        onehot_encoder \u003d OneHotEncoder(inputCol\u003d col + \"_index\", outputCol\u003d col + \"_onehot\")\n        df \u003d onehot_encoder.transform(df)\n        \n        df.drop(col, col + \"_index\")\n        df.withColumnRenamed(col + \"_onehot\", col)\n   \n    return df",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 9:12:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493509633415_464751883",
      "id": "20170429-194713_2101292374",
      "dateCreated": "Apr 29, 2017 7:47:13 PM",
      "dateStarted": "Apr 29, 2017 8:55:37 PM",
      "dateFinished": "Apr 29, 2017 8:55:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create assembler for shaping",
      "text": "%pyspark\nfrom pyspark.ml.feature import VectorAssembler\nvector_assembler \u003d VectorAssembler(\n    inputCols\u003d[\u0027AdID\u0027, \u0027QueryID\u0027, \u0027DisplayURL\u0027, \u0027AdvertiserID\u0027, \u0027Depth\u0027, \u0027Position\u0027, \u0027KeywordID\u0027, \u0027TitleID\u0027, \u0027DescriptionID\u0027, \u0027UserID\u0027, \u0027Impression\u0027, \u0027Click\u0027],\n    outputCol\u003d\"features\")",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:47:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493004928721_-1782805492",
      "id": "20170423-233528_1657420097",
      "dateCreated": "Apr 23, 2017 11:35:28 PM",
      "dateStarted": "Apr 29, 2017 8:47:09 PM",
      "dateFinished": "Apr 29, 2017 8:47:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shape training dataset",
      "text": "%pyspark\ntraining_top25K_normCTR_encoded \u003d stringindexonehotencode(training_top25K_normCTR)\ntraining_top25K_normCTR_encoded",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:55:47 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-3983876986788056661.py\", line 337, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"\u003cstdin\u003e\", line 6, in stringindexonehotencode\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/base.py\", line 64, in fit\n    return self._fit(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 236, in _fit\n    java_model \u003d self._fit_java(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 233, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1339.fit.\n: org.apache.spark.SparkException: Job 123 cancelled part of cancelled job group zeppelin-20170423-232058_1855626306\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:374)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1203)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1203)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1202)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:92)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493004058607_-394017602",
      "id": "20170423-232058_1855626306",
      "dateCreated": "Apr 23, 2017 11:20:58 PM",
      "dateStarted": "Apr 29, 2017 8:55:47 PM",
      "dateFinished": "Apr 29, 2017 9:11:48 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 8:07:17 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493510837894_-940415163",
      "id": "20170429-200717_1076230853",
      "dateCreated": "Apr 29, 2017 8:07:17 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save training dataset for ML",
      "text": "%pyspark\ntraining_top25K_normCTR_assembled.write.options(header\u003d\"true\").parquet(os.path.join(dir_name ,\"training_top25K_normCTR_assembled\"), \"overwrite\")",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 12:26:09 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493262445431_582803450",
      "id": "20170426-230725_327672313",
      "dateCreated": "Apr 26, 2017 11:07:25 PM",
      "dateStarted": "Apr 27, 2017 12:26:09 AM",
      "dateFinished": "Apr 27, 2017 12:42:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shape test dataset",
      "text": "%pyspark\ntest_assembled \u003d vector_assembler.transform(test).drop(\u0027AdID\u0027, \u0027QueryID\u0027, \u0027DisplayURL\u0027, \u0027AdvertiserID\u0027, \u0027Depth\u0027, \u0027Position\u0027, \u0027KeywordID\u0027, \u0027TitleID\u0027, \u0027DescriptionID\u0027, \u0027UserID\u0027,\u0027Click\u0027,\u0027Impression\u0027)\ntest_assembled",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 12:43:08 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493004744302_-1765215956",
      "id": "20170423-233224_1767140216",
      "dateCreated": "Apr 23, 2017 11:32:24 PM",
      "dateStarted": "Apr 27, 2017 12:43:09 AM",
      "dateFinished": "Apr 27, 2017 12:43:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save test dataset for ML",
      "text": "%pyspark\ntest_assembled.write.options(header\u003d\"true\").parquet(os.path.join(dir_name ,\"test_assembled\"), \"overwrite\")",
      "user": "anonymous",
      "dateUpdated": "Apr 27, 2017 12:43:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493003770167_-1172378306",
      "id": "20170423-231610_1803929984",
      "dateCreated": "Apr 23, 2017 11:16:10 PM",
      "dateStarted": "Apr 27, 2017 12:43:11 AM",
      "dateFinished": "Apr 27, 2017 12:56:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "Apr 26, 2017 11:08:15 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493262495027_1865833762",
      "id": "20170426-230815_1167601179",
      "dateCreated": "Apr 26, 2017 11:08:15 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "pCTR/HW2",
  "id": "2CGDKZJCG",
  "angularObjects": {
    "2CF6E122K:shared_process": [],
    "2CH1G59ET:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}