{
  "paragraphs": [
    {
      "text": "%md\n# CS 5304 Assignments #2\nBy Chris M Wang\n\nWe consider data used to predict the click-through-rate (pCTR) of online ads. An accurate model is necessary in the search advertising market in order to appropriately rank ads and price clicks. The data contains 11 variables and 1 output, corresponding to the number of times a given ad was clicked by the user among the number of times it was displayed.\nFor each instance (training example), the input variables serve to classify various properties of the ad displayed, in addition to the specific search query entered. The identifiers include unique identifiers for each query, ad, keyword, advertiser, title, description, display url, user, ad position, and ad depth (further details available in the KDD documentation).",
      "user": "anonymous",
      "dateUpdated": "May 30, 2017 5:33:05 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eCS 5304 Assignments #2\u003c/h1\u003e\n\u003cp\u003eBy Chris M Wang\u003c/p\u003e\n\u003cp\u003eWe consider data used to predict the click-through-rate (pCTR) of online ads. An accurate model is necessary in the search advertising market in order to appropriately rank ads and price clicks. The data contains 11 variables and 1 output, corresponding to the number of times a given ad was clicked by the user among the number of times it was displayed.\u003cbr/\u003eFor each instance (training example), the input variables serve to classify various properties of the ad displayed, in addition to the specific search query entered. The identifiers include unique identifiers for each query, ad, keyword, advertiser, title, description, display url, user, ad position, and ad depth (further details available in the KDD documentation).\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493005023932_-293168309",
      "id": "20170423-233703_1256560744",
      "dateCreated": "Apr 23, 2017 11:37:03 PM",
      "dateStarted": "May 30, 2017 5:33:05 PM",
      "dateFinished": "May 30, 2017 5:33:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize",
      "text": "%pyspark\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions\nfrom __future__ import division\n\nimport os.path\ndir_name \u003d \"/Users/kylin1989/Downloads/data\"\n\nspark \u003d SparkSession.builder.appName(\"pCTR\").config(\"spark.sql.broadcastTimeout\", \"6000\").getOrCreate()",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:46:26 PM",
      "config": {
        "lineNumbers": false,
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 464.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1492892844428_-2054507981",
      "id": "20170421-215501_851260905",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 30, 2017 4:46:11 PM",
      "dateFinished": "Apr 30, 2017 4:46:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load descriptionid_tokensid.txt",
      "text": "%pyspark\ndescriptionid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"descriptionid_tokensid.txt\"))\ndescriptionid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:56 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492916966724_-1680822677",
      "id": "20170422-230926_1549676367",
      "dateCreated": "Apr 22, 2017 11:09:26 PM",
      "dateStarted": "Apr 27, 2017 2:31:34 PM",
      "dateFinished": "Apr 27, 2017 2:31:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load purchasedkeywordid_tokensid.txt",
      "text": "%pyspark\npurchasedkeywordid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"purchasedkeywordid_tokensid.txt\"))\npurchasedkeywordid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:56 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917810620_-417484601",
      "id": "20170422-232330_2027512770",
      "dateCreated": "Apr 22, 2017 11:23:30 PM",
      "dateStarted": "Apr 23, 2017 10:37:28 PM",
      "dateFinished": "Apr 23, 2017 10:37:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load queryid_tokensid.tx",
      "text": "%pyspark\nqueryid_tokensid \u003d spark.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"queryid_tokensid.txt\"))\nqueryid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:56 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": false,
        "editorSetting": {
          "language": "python"
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492892844434_-2042965514",
      "id": "20170422-142536_891748670",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 23, 2017 10:37:27 PM",
      "dateFinished": "Apr 23, 2017 10:37:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": " Load titleid_tokensid.txt",
      "text": "%pyspark\ntitleid_tokensid \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"titleid_tokensid.txt\"))\ntitleid_tokensid",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:57 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917677429_-311060753",
      "id": "20170422-232117_650626617",
      "dateCreated": "Apr 22, 2017 11:21:17 PM",
      "dateStarted": "Apr 23, 2017 10:37:32 PM",
      "dateFinished": "Apr 23, 2017 10:37:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load userid_profile.txt",
      "text": "%pyspark\nuserid_profile \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(delimiter\u003d\"\\t\").load(os.path.join(dir_name, \"userid_profile.txt\"))\nuserid_profile",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:57 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[_c0: string, _c1: string, _c2: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492917685047_88860275",
      "id": "20170422-232125_1561285407",
      "dateCreated": "Apr 22, 2017 11:21:25 PM",
      "dateStarted": "Apr 23, 2017 10:37:26 PM",
      "dateFinished": "Apr 23, 2017 10:37:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load training.txt",
      "text": "%pyspark\ntraining \u003d sqlContext.read.format(\"com.databricks.spark.csv\").options(header\u003d\"false\", inferSchema\u003d\"true\", delimiter\u003d\"\\t\").load(os.path.join(dir_name,\"training.txt\"))\n\ntraining \u003d training.withColumnRenamed(\u0027_c0\u0027, \u0027Click\u0027).withColumnRenamed(\u0027_c1\u0027, \u0027Impression\u0027).withColumnRenamed(\u0027_c2\u0027, \u0027DisplayURL\u0027).withColumnRenamed(\u0027_c3\u0027, \u0027AdID\u0027).withColumnRenamed(\u0027_c4\u0027, \u0027AdvertiserID\u0027).withColumnRenamed(\u0027_c5\u0027, \u0027Depth\u0027).withColumnRenamed(\u0027_c6\u0027, \u0027Position\u0027).withColumnRenamed(\u0027_c7\u0027, \u0027QueryID\u0027).withColumnRenamed(\u0027_c8\u0027, \u0027KeywordID\u0027).withColumnRenamed(\u0027_c9\u0027, \u0027TitleID\u0027).withColumnRenamed(\u0027_c10\u0027, \u0027DescriptionID\u0027).withColumnRenamed(\u0027_c11\u0027, \u0027UserID\u0027)\ntraining",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:46:32 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python"
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[Click: int, Impression: int, DisplayURL: decimal(20,0), AdID: int, AdvertiserID: int, Depth: int, Position: int, QueryID: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492892844431_-2054123232",
      "id": "20170421-221742_2093965047",
      "dateCreated": "Apr 22, 2017 4:27:24 PM",
      "dateStarted": "Apr 30, 2017 4:46:32 PM",
      "dateFinished": "Apr 30, 2017 4:52:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load test.txt",
      "text": "%pyspark\ntest \u003d spark.read.format(\"com.databricks.spark.csv\").options(header\u003d\"false\", inferSchema\u003d\"true\", delimiter\u003d\"\\t\").load(os.path.join(dir_name,\"test.txt\"))\ntest \u003d training.withColumnRenamed(\u0027_c0\u0027, \u0027DisplayURL\u0027).withColumnRenamed(\u0027_c1\u0027, \u0027AdID\u0027).withColumnRenamed(\u0027_c2\u0027, \u0027AdvertiserID\u0027).withColumnRenamed(\u0027_c3\u0027, \u0027Depth\u0027).withColumnRenamed(\u0027_c4\u0027, \u0027Position\u0027).withColumnRenamed(\u0027_c5\u0027, \u0027QueryID\u0027).withColumnRenamed(\u0027_c6\u0027, \u0027KeywordID\u0027).withColumnRenamed(\u0027_c7\u0027, \u0027TitleID\u0027).withColumnRenamed(\u0027_c8\u0027, \u0027DescriptionID\u0027).withColumnRenamed(\u0027_c9\u0027, \u0027UserID\u0027)\ntest",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 6:25:13 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python"
        },
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[Click: int, Impression: int, DisplayURL: decimal(20,0), AdID: int, AdvertiserID: int, Depth: int, Position: int, QueryID: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493000444586_2092285630",
      "id": "20170423-222044_532393071",
      "dateCreated": "Apr 23, 2017 10:20:44 PM",
      "dateStarted": "Apr 30, 2017 6:25:13 PM",
      "dateFinished": "Apr 30, 2017 6:25:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 2 (non-Big-Data)\nidentify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 2 (non-Big-Data)\u003c/h2\u003e\n\u003cp\u003eidentify the top 25,000 instances with the same ad id and query by frequency from the training set. Select all instances using these ad id/query id combos for the training set. Only operate on this subset of the training for the remainder of the assignment. Operate on the full test set for the remainder of the assignment.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975190465_589883785",
      "id": "20170423-151950_157799325",
      "dateCreated": "Apr 23, 2017 3:19:50 PM",
      "dateStarted": "Apr 29, 2017 10:53:58 PM",
      "dateFinished": "Apr 29, 2017 10:53:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Identify the top25k AdID-QueryID combination by the frequence of instances in the training dataset",
      "text": "%pyspark\ntraining_top25k_AdID_QueryID \u003d training.groupBy(\"AdID\", \"QueryID\").count().sort(desc(\"count\")).limit(25000)\ntraining_top25k_AdID_QueryID\n# print(training_top25k_AdID_QueryID.count())",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:53:56 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, count: bigint]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492918411251_-223496179",
      "id": "20170422-233331_1614972558",
      "dateCreated": "Apr 22, 2017 11:33:31 PM",
      "dateStarted": "Apr 30, 2017 4:53:56 PM",
      "dateFinished": "Apr 30, 2017 4:53:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Filter the training using the identified the top25k AdID-QueryID combinations",
      "text": "%pyspark\ntraining_top25K \u003d training.join(training_top25k_AdID_QueryID.drop(\"count\"), [\u0027AdID\u0027, \u0027QueryID\u0027], \u0027inner\u0027)\ntraining_top25K",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:53:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975731673_-1468556768",
      "id": "20170423-152851_2110849751",
      "dateCreated": "Apr 23, 2017 3:28:51 PM",
      "dateStarted": "Apr 30, 2017 4:53:58 PM",
      "dateFinished": "Apr 30, 2017 4:53:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 3 \nCompute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:58 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 3\u003c/h2\u003e\n\u003cp\u003eCompute a position and depth normalized click-through-rate for each identifier, as well as combinations (conjunctions) of these identifiers.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492975732832_-646348368",
      "id": "20170423-152852_1407717236",
      "dateCreated": "Apr 23, 2017 3:28:52 PM",
      "dateStarted": "Apr 29, 2017 10:53:58 PM",
      "dateFinished": "Apr 29, 2017 10:53:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Add CTR column",
      "text": "%pyspark\ntraining_top25K_CTR \u003d training_top25K.withColumn(\u0027CTR\u0027, training_top25K.Click/training_top25K.Impression)\ntraining_top25K_CTR",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:54:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int, CTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492996033107_-2032875988",
      "id": "20170423-210713_754289250",
      "dateCreated": "Apr 23, 2017 9:07:13 PM",
      "dateStarted": "Apr 30, 2017 4:54:00 PM",
      "dateFinished": "Apr 30, 2017 4:54:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create NormCTR column",
      "text": "%pyspark\ntraining_top25K_normCTR \u003d training_top25K_CTR.withColumn(\u0027normCTR\u0027, training_top25K_CTR.CTR * training_top25K_CTR.Position / training_top25K_CTR.Depth).drop(\"CTR\")\ntraining_top25K_normCTR",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:54:04 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[AdID: int, QueryID: int, Click: int, Impression: int, DisplayURL: decimal(20,0), AdvertiserID: int, Depth: int, Position: int, KeywordID: int, TitleID: int, DescriptionID: int, UserID: int, normCTR: double]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493256443156_-276304665",
      "id": "20170426-212723_821672347",
      "dateCreated": "Apr 26, 2017 9:27:23 PM",
      "dateStarted": "Apr 30, 2017 4:54:04 PM",
      "dateFinished": "Apr 30, 2017 4:54:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 4\nAnnotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:58 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 4\u003c/h2\u003e\n\u003cp\u003eAnnotate each instance in the training and testing set with the normalized click through rates. Submit these 2 data sets with the code for parts 1 – 4.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492995915775_790241231",
      "id": "20170423-210515_1452637488",
      "dateCreated": "Apr 23, 2017 9:05:15 PM",
      "dateStarted": "Apr 29, 2017 10:53:59 PM",
      "dateFinished": "Apr 29, 2017 10:53:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save training dataset for ML",
      "text": "%pyspark\ntraining_top25K_normCTR.write.options(header\u003d\"true\").parquet(os.path.join(dir_name,\"training_top25K_normCTR\"))",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:54:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493003743413_-1144707034",
      "id": "20170423-231543_1345825894",
      "dateCreated": "Apr 23, 2017 11:15:43 PM",
      "dateStarted": "Apr 29, 2017 8:17:59 PM",
      "dateFinished": "Apr 29, 2017 8:46:46 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load training dataset if needed",
      "text": "%pyspark\ntraining_top25K_normCTR \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"training_top25K_normCTR\"))\ntraining_top25K_normCTR",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:43:19 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[normCTR: double, features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493510593933_-904385039",
      "id": "20170429-200313_553302843",
      "dateCreated": "Apr 29, 2017 8:03:13 PM",
      "dateStarted": "Apr 29, 2017 8:03:56 PM",
      "dateFinished": "Apr 29, 2017 8:04:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load test dataset if needed",
      "text": "%pyspark\ntest \u003d spark.read.format(\"parquet\").options(header\u003d\"true\", inferSchema\u003d\"true\").load(os.path.join(dir_name, \"test\"))\ntest",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493510925277_73316111",
      "id": "20170429-200845_367382228",
      "dateCreated": "Apr 29, 2017 8:08:45 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## PART 5\nShape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePART 5\u003c/h2\u003e\n\u003cp\u003eShape the data into feature vectors suitable for input into machine learning. Describe a selected learning model and the shaping selected using empirical analysis of the data. Note that you may select any model type from the Spark library and your submission will not be judged by whether your selected model turns out to be the best model for the problem. For assignment #2, you DO NOT need to implement the machine learning model. Submit the code and the training/testing data sets generated for Part 5.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492977005280_504455850",
      "id": "20170423-155005_46408624",
      "dateCreated": "Apr 23, 2017 3:50:05 PM",
      "dateStarted": "Apr 29, 2017 10:53:59 PM",
      "dateFinished": "Apr 29, 2017 10:53:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Answer\nI select the Linear Regression model for the following reasons:\n* the pCTR is a float, this is a regression problem\n* logistic regression has a decent performance",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:53:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eAnswer\u003c/h3\u003e\n\u003cp\u003eI select the Linear Regression model for the following reasons:\u003cbr/\u003e* the pCTR is a float, this is a regression problem\u003cbr/\u003e* logistic regression has a decent performance\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1492995934848_-1688095001",
      "id": "20170423-210534_372251776",
      "dateCreated": "Apr 23, 2017 9:05:34 PM",
      "dateStarted": "Apr 29, 2017 10:54:00 PM",
      "dateFinished": "Apr 29, 2017 10:54:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create string indexer and one-hot-encoder",
      "text": "%pyspark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\ndef stringindexonehotencode(df):\n    # string_cols \u003d [\"DisplayURL\", \"AdID\", \"AdvertiserID\", \"QueryID\", \"KeywordID\",  \"TitleID\", \"DescriptionID\", \"UserID\"]\n    string_cols \u003d [ \"AdID\", \"QueryID\",\"UserID\"]\n\n    for col in string_cols:\n        # string_indexer \u003d StringIndexer(inputCol\u003dcol, outputCol\u003d(col + \"_index\"))\n        # string_indexer.fit(df)\n        # df \u003d str_indexer.transform(df)\n        # onehot_encoder \u003d OneHotEncoder(inputCol\u003d col + \"_index\", outputCol\u003d col + \"_onehot\")\n        onehot_encoder \u003d OneHotEncoder(inputCol\u003d col, outputCol\u003d col + \"_onehot\")\n\n        df \u003d onehot_encoder.transform(df)\n        df.drop(col).withColumnRenamed(col + \"_onehot\", col)\n   \n    return df",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 7:48:10 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493509633415_464751883",
      "id": "20170429-194713_2101292374",
      "dateCreated": "Apr 29, 2017 7:47:13 PM",
      "dateStarted": "Apr 30, 2017 4:54:15 PM",
      "dateFinished": "Apr 30, 2017 4:54:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create assembler for shaping",
      "text": "%pyspark\nfrom pyspark.ml.feature import VectorAssembler\nvector_assembler \u003d VectorAssembler(\n    inputCols\u003d[\u0027AdID\u0027, \u0027QueryID\u0027, \u0027DisplayURL\u0027, \u0027AdvertiserID\u0027, \u0027Depth\u0027, \u0027Position\u0027, \u0027KeywordID\u0027, \u0027TitleID\u0027, \u0027DescriptionID\u0027, \u0027UserID\u0027, \u0027Impression\u0027, \u0027Click\u0027],\n    outputCol\u003d\"features\")",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 4:54:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493004928721_-1782805492",
      "id": "20170423-233528_1657420097",
      "dateCreated": "Apr 23, 2017 11:35:28 PM",
      "dateStarted": "Apr 30, 2017 4:54:17 PM",
      "dateFinished": "Apr 30, 2017 4:54:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shape training dataset (\u0027GC overhead limit exceeded\u0027)",
      "text": "%pyspark\ntraining_top25K_normCTR_encoded \u003d stringindexonehotencode(training_top25K_normCTR)\ntraining_top25K_normCTR_encoded",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 7:48:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-4183470334946035358.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-4183470334946035358.py\", line 337, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"\u003cstdin\u003e\", line 6, in stringindexonehotencode\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/base.py\", line 105, in transform\n    return self._transform(dataset)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/ml/wrapper.py\", line 252, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/Applications/zeppelin-0.7.1-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o385.transform.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Integer.valueOf(Integer.java:832)\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:65)\n\tat scala.Array$.tabulate(Array.scala:331)\n\tat org.apache.spark.ml.feature.OneHotEncoder.transform(OneHotEncoder.scala:157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493004058607_-394017602",
      "id": "20170423-232058_1855626306",
      "dateCreated": "Apr 23, 2017 11:20:58 PM",
      "dateStarted": "Apr 30, 2017 4:54:19 PM",
      "dateFinished": "Apr 30, 2017 5:13:22 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save training dataset for ML",
      "text": "%pyspark\ntraining_top25K_normCTR_encoded.write.options(header\u003d\"true\").parquet(os.path.join(dir_name,\"training_top25K_normCTR_encoded\"))",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 5:29:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493585675776_1023745107",
      "id": "20170430-165435_258121762",
      "dateCreated": "Apr 30, 2017 4:54:35 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Assemble traning dataset",
      "text": "%pyspark\ntraining_top25K_normCTR_assembled \u003d vector_assembler.transform(training_top25K_normCTR_encoded)\ntraining_top25K_normCTR_assembled",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 11:15:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493522096976_1437617548",
      "id": "20170429-231456_1042971800",
      "dateCreated": "Apr 29, 2017 11:14:56 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save training dataset for ML",
      "text": "%pyspark\ntraining_top25K_normCTR_assembled.write.options(header\u003d\"true\").parquet(os.path.join(dir_name ,\"training_top25K_normCTR_assembled\"), \"overwrite\")",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2017 10:54:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-8186053028019748147.py\", line 349, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/var/folders/sr/4y_4zrx51k3_8flpmj5683tm0000gq/T/zeppelin_pyspark-8186053028019748147.py\", line 342, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027training_top25K_normCTR_assembled\u0027 is not defined\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493262445431_582803450",
      "id": "20170426-230725_327672313",
      "dateCreated": "Apr 26, 2017 11:07:25 PM",
      "dateStarted": "Apr 29, 2017 11:01:06 PM",
      "dateFinished": "Apr 29, 2017 11:16:50 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shape test dataset",
      "text": "%pyspark\ntest_assembled \u003d vector_assembler.transform(test).drop(\u0027AdID\u0027, \u0027QueryID\u0027, \u0027DisplayURL\u0027, \u0027AdvertiserID\u0027, \u0027Depth\u0027, \u0027Position\u0027, \u0027KeywordID\u0027, \u0027TitleID\u0027, \u0027DescriptionID\u0027, \u0027UserID\u0027,\u0027Click\u0027,\u0027Impression\u0027)\ntest_assembled",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 6:26:36 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[features: vector]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1493004744302_-1765215956",
      "id": "20170423-233224_1767140216",
      "dateCreated": "Apr 23, 2017 11:32:24 PM",
      "dateStarted": "Apr 30, 2017 6:26:37 PM",
      "dateFinished": "Apr 30, 2017 6:26:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Save test dataset for ML",
      "text": "%pyspark\ntest_assembled.write.options(header\u003d\"true\").parquet(os.path.join(dir_name ,\"test_assembled\"), \"overwrite\")",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 6:26:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1493003770167_-1172378306",
      "id": "20170423-231610_1803929984",
      "dateCreated": "Apr 23, 2017 11:16:10 PM",
      "dateStarted": "Apr 30, 2017 6:26:40 PM",
      "dateFinished": "Apr 30, 2017 6:40:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "Apr 30, 2017 7:47:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1493262495027_1865833762",
      "id": "20170426-230815_1167601179",
      "dateCreated": "Apr 26, 2017 11:08:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "pCTR/HW2",
  "id": "2CGDKZJCG",
  "angularObjects": {
    "2CF6E122K:shared_process": [],
    "2CH1G59ET:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}